{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "006eb558",
   "metadata": {},
   "source": [
    "# 데이터 다운로드 하기\n",
    ":인터넷에서 지정된 파일을 내 pc에 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e02461a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# library\n",
    "import urllib.request\n",
    "\n",
    "# URL과 저장경로 지정하기\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"./Data/test.png\"\n",
    "\n",
    "# 다운로드 하기\n",
    "urllib.request.urlretrieve(url, savename)\n",
    "print(\"저장 되었습니다.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "138f97fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# library\n",
    "import urllib.request\n",
    "\n",
    "# URL과 저장경로 지정하기\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\" b\n",
    "savename = \"./Data/test1.png\"\n",
    "\n",
    "# 다운로드 하기\n",
    "mem = urllib.request.urlopen(url).read()\n",
    "\n",
    "# 파일로 저장하기\n",
    "with open(savename, \"wb\") as f:\n",
    "    f.write(mem)\n",
    "print(\"저장 되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2948b1",
   "metadata": {},
   "source": [
    "# BeatifulSoup로 Scraping하기\n",
    ": 간단하게 HTML과 XML에서 정보를 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9883fd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\chanholee\\anaconda3\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\chanholee\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6ae062d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>Header</h1>\n",
      "<p> Line 1을 서술 </p>\n",
      "------------\n",
      "h1= Header\n",
      "h1= Header\n",
      "p1  Line 1을 서술 \n",
      "p1  Line 1을 서술 \n"
     ]
    }
   ],
   "source": [
    "# 기본 사용법\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample \n",
    "\n",
    "html = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <h1>Header</h1>\n",
    "                <p> Line 1을 서술 </p>\n",
    "            </body>\n",
    "        </html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML분석\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "#print(soup)\n",
    "\n",
    "# 원하는 부분 추출하기 (태그를 통한 스크래핑)\n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "print(h1)\n",
    "print(p1)\n",
    "print(\"------------\")\n",
    "# Text만 출력\n",
    "print(\"h1=\", h1.string)\n",
    "print(\"h1=\", h1.text)\n",
    "print(\"p1\", p1.string)\n",
    "print(\"p1\", p1.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e9b5a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 id=\"title\"> Header </h1>\n",
      "<p1 id=\"body\"> Line 1을 서술 \n",
      "</p1>\n",
      "title=  Header \n",
      "body=  Line 1을 서술 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# id로 요소를 추출하기\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample\n",
    "\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <h1 id=\"title\"> Header </h1>\n",
    "            <p1 id=\"body\"> Line 1을 서술 </p>\n",
    "        </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML분석\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 원하는 부분 추출하기\n",
    "title = soup.find(id='title')\n",
    "body = soup.find(id=\"body\")\n",
    "print(title)\n",
    "print(body)\n",
    "\n",
    "# Text만 출력\n",
    "print(\"title=\", title.text)\n",
    "print(\"body=\", body.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79713688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver >>> http://www.naver.com\n",
      "daum >>> http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "# 여러개의 요소 추출하기\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample\n",
    "\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <ul>\n",
    "                <li><a href=\"http://www.naver.com\">naver</a></li>\n",
    "                <li><a href=\"http://www.daum.net\">daum</a></li>\n",
    "            </ul>\n",
    "        </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "# HTML분석\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 원하는 부분 추출하기\n",
    "# soup.find('a')\n",
    "links = soup.find_all('a')\n",
    "\n",
    "# 링크 목록 출력\n",
    "for link in links:\n",
    "#   print(link.string)\n",
    "#    print(link.attrs['href']) #attrs = attribute 태그\n",
    "    \n",
    "    href = link.attrs['href']\n",
    "    text = link.string\n",
    "    print(text, \">>>\", href)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ad087aea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기상청 육상 중기예보\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-d2ead2f6855c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# 데이터 정리하기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mlist_wfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwfs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mexcept_char\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'<'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\">\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "# 기상청 자료 활용하기\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109\"\n",
    "\n",
    "# data 가져오기\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "# print(soup)\n",
    "\n",
    "\n",
    "# 원하는 데이터 추출하기\n",
    "title = soup.find('title').string\n",
    "print(title)\n",
    "#wf = str(soup.find('wf').string)\n",
    "wf = soup.find('wf').string\n",
    "\n",
    "# 데이터 정리하기\n",
    "list_wfs = list(wf)\n",
    "print(list_wfs)\n",
    "except_char = ['<','b','r','/',\">\"]\n",
    "result = \"\"\n",
    "\n",
    "for lwf in list_wfs:\n",
    "    if lwf not in except_char:\n",
    "        result += lwf\n",
    "print('-'*100)\n",
    "print(result)\n",
    "\n",
    "\n",
    "#wlist = wf.split('<br />')\n",
    "\n",
    "#print(wlist)\n",
    "#for w in wlist:\n",
    "#    print(w)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda411b3",
   "metadata": {},
   "source": [
    "# css 선택자 사용하기\n",
    "soup.select_one() : css선택자로 요소 하나를 추출.\n",
    "soup.select() : css선택자로 요소 여러개를 리스트로 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "74149b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <div id = \"meigen\">\n",
    "                <h1>위키 북스<h1>\n",
    "                <ul class=\"items\">\n",
    "                    <li>유니티 게임 이펙트 입문서</li>\n",
    "                    <li>모던 웹사이트 디자인의 정석</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "        </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 필요한 부분 css로 추출하기\n",
    "# 타이틀 부분 추출하기\n",
    "\n",
    "h1 = soup.select_one(\"div#meigen > h1\").string  # id:# , class:.  , > : 자손.  \" \" : 후손\n",
    "print(h1)\n",
    "\n",
    "# 목록 부분 추출하기\n",
    "li_lists = soup.select(\"div#meigen > ul.items > li\")\n",
    "print(li_lists)\n",
    "for li in li_lists:\n",
    "    print(li.string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faeab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 금융에서 환율 정보 추출하기\n",
    "# https://finance.naver.com/marketindex/\n",
    "# #exchangeList > li.on > a.head.usd > div > span.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "09d47e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usd / krw :  1,143.50\n"
     ]
    }
   ],
   "source": [
    "# 미국 환율 가져오기\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# HTML\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "#print(soup)\n",
    "\n",
    "#데이터 추출하기\n",
    "price = soup.select_one(\"div.head_info > span.value\").string\n",
    "print(\"usd / krw : \", price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "54fb0b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미국 :  1,144.00\n",
      "일본 :  1,036.42\n",
      "유럽연합 :  1,358.16\n",
      "중국 :  176.85\n"
     ]
    }
   ],
   "source": [
    "# id:# , class:.  , > : 자손.  \" \" : 후손\n",
    "\n",
    "# 미국, 일본, 유렵연합, 중국의 환율\n",
    "# HTML\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "#print(soup)\n",
    "\n",
    "#데이터 추출하기\n",
    "prices = soup.select(\"div.head_info > span.value\")\n",
    "\n",
    "#for price in prices:\n",
    "#   print(price.string)\n",
    "print(\"미국 : \", prices[0].string)\n",
    "print(\"일본 : \", prices[1].string)\n",
    "print(\"유럽연합 : \", prices[2].string)\n",
    "print(\"중국 : \", prices[3].string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4b120f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하늘과 바람과 별과 시\n",
      "서시\n",
      "자화상\n",
      "소년\n",
      "눈 오는 지도\n",
      "돌아와 보는 밤\n",
      "병원\n",
      "새로운 길\n",
      "간판 없는 거리\n",
      "태초의 아침\n",
      "또 태초의 아침\n",
      "새벽이 올 때까지\n",
      "무서운 시간\n",
      "십자가\n",
      "바람이 불어\n",
      "슬픈 족속\n",
      "눈감고 간다\n",
      "또 다른 고향\n",
      "길\n",
      "별 헤는 밤\n",
      "흰 그림자\n",
      "사랑스런 추억\n",
      "흐르는 거리\n",
      "쉽게 씌어진 시\n",
      "봄\n",
      "참회록\n",
      "간(肝)\n",
      "위로\n",
      "팔복\n",
      "못자는밤\n",
      "달같이\n",
      "고추밭\n",
      "아우의 인상화\n",
      "사랑의 전당\n",
      "이적\n",
      "비오는 밤\n",
      "산골물\n",
      "유언\n",
      "창\n",
      "바다\n",
      "비로봉\n",
      "산협의 오후\n",
      "명상\n",
      "소낙비\n",
      "한난계\n",
      "풍경\n",
      "달밤\n",
      "장\n",
      "밤\n",
      "황혼이 바다가 되어\n",
      "아침\n",
      "빨래\n",
      "꿈은 깨어지고\n",
      "산림\n",
      "이런날\n",
      "산상\n",
      "양지쪽\n",
      "닭\n",
      "가슴 1\n",
      "가슴 2\n",
      "비둘기\n",
      "황혼\n",
      "남쪽 하늘\n",
      "창공\n",
      "거리에서\n",
      "삶과 죽음\n",
      "초한대\n",
      "산울림\n",
      "해바라기 얼굴\n",
      "귀뚜라미와 나와\n",
      "애기의 새벽\n",
      "햇빛·바람\n",
      "반디불\n",
      "둘 다\n",
      "거짓부리\n",
      "눈\n",
      "참새\n",
      "버선본\n",
      "편지\n",
      "봄\n",
      "무얼 먹구 사나\n",
      "굴뚝\n",
      "햇비\n",
      "빗자루\n",
      "기왓장 내외\n",
      "오줌싸개 지도\n",
      "병아리\n",
      "조개껍질\n",
      "겨울\n",
      "트루게네프의 언덕\n",
      "달을 쏘다\n",
      "별똥 떨어진 데\n",
      "화원에 꽃이 핀다\n",
      "종시\n"
     ]
    }
   ],
   "source": [
    "# 윤동주 시안 작품 가져오기\n",
    "# #mw-content-text > div.mw-parser-output > ul:nth-child(6) > li > b > a\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# HTML\n",
    "url = \"https://ko.wikisource.org/wiki/%EC%A0%80%EC%9E%90:%EC%9C%A4%EB%8F%99%EC%A3%BC\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "#print(soup)\n",
    "\n",
    "#데이터 추출하기\n",
    "a_list= soup.select('#mw-content-text > div.mw-parser-output > ul > li a')\n",
    "#print(a_list)\n",
    "\n",
    "for a in a_list:\n",
    "    if a.string == \"증보판\":\n",
    "        continue             # 무시하고 진행응 continue\n",
    "    print(a.string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dc8ae9",
   "metadata": {},
   "source": [
    "## 다음 영화 연간 순위 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "48aced45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : 남산의 부장들\n",
      "2 : 다만 악에서 구하소서\n",
      "3 : 반도\n",
      "4 : 히트맨\n",
      "5 : 테넷\n",
      "6 : 백두산\n",
      "7 : #살아있다\n",
      "8 : 강철비2: 정상회담\n",
      "9 : 담보\n",
      "10 : 닥터 두리틀\n",
      "11 : 삼진그룹 영어토익반\n",
      "12 : 정직한 후보\n",
      "13 : 도굴\n",
      "14 : 클로젯\n",
      "15 : 오케이 마담\n",
      "16 : 해치지않아\n",
      "17 : 천문: 하늘에 묻는다\n",
      "18 : 결백\n",
      "19 : 1917\n",
      "20 : 작은 아씨들\n",
      "21 : 미드웨이\n",
      "22 : 시동\n",
      "23 : 지푸라기라도 잡고 싶은 짐승들\n",
      "24 : 미스터 주: 사라진 VIP\n",
      "25 : 인비저블맨\n",
      "26 : 나쁜 녀석들: 포에버\n",
      "27 : 국제수사\n",
      "28 : 침입자\n",
      "29 : 스타워즈: 라이즈 오브 스카이워커\n",
      "30 : 스파이 지니어스 \n",
      "31 : 이웃사촌\n",
      "32 : 온워드: 단 하루의 기적\n",
      "33 : 소리도 없이\n",
      "34 : 버즈 오브 프레이(할리 퀸의 황홀한 해방)\n",
      "35 : 원더 우먼 1984\n",
      "36 : 겨울왕국 2\n",
      "37 : 오! 문희\n",
      "38 : 그린랜드\n",
      "39 : 위대한 쇼맨\n",
      "40 : 런\n",
      "41 : 뮬란\n",
      "42 : 내가 죽던 날\n",
      "43 : 기생충\n",
      "44 : 신비아파트 극장판 하늘도깨비 대 요르문간드\n",
      "45 : 프리즌 이스케이프\n",
      "46 : 검객\n",
      "47 : 조제\n",
      "48 : 사라진 시간\n",
      "49 : 밤쉘: 세상을 바꾼 폭탄선언\n",
      "50 : 알라딘\n"
     ]
    }
   ],
   "source": [
    "#https://movie.daum.net/boxoffice/yearly?year=2021\n",
    "# #mainContent > div > div.box_boxoffice > ol > li:nth-child(1) > div > div.thumb_cont > strong > a\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "\n",
    "# HTML\n",
    "url = \"https://movie.daum.net/boxoffice/yearly?year=2021\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML분석\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "#print(soup)\n",
    "b_list = soup.select('#mainContent > div > div.box_boxoffice > ol > li > div > div.thumb_cont > strong > a')\n",
    "#print(b_list)\n",
    "n = 1\n",
    "for b in b_list:\n",
    "    print(n, \":\", b.string)\n",
    "    n+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a415fbf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8930a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55629423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd558c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
