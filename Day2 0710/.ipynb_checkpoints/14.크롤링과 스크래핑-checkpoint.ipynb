{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f251626",
   "metadata": {},
   "source": [
    "# 데이터 다운로드 하기\n",
    ":인터넷에서 지정된 파일을 내 pc에 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a47fdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# library\n",
    "import urllib.request\n",
    "\n",
    "# URL과 저장경로 지정하기\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"./Data/test.png\"\n",
    "\n",
    "# 다운로드 하기\n",
    "urllib.request.urlretrieve(url, savename)\n",
    "print(\"저장 되었습니다.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe83ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# library\n",
    "import urllib.request\n",
    "\n",
    "# URL과 저장경로 지정하기\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\" b\n",
    "savename = \"./Data/test1.png\"\n",
    "\n",
    "# 다운로드 하기\n",
    "mem = urllib.request.urlopen(url).read()\n",
    "\n",
    "# 파일로 저장하기\n",
    "with open(savename, \"wb\") as f:\n",
    "    f.write(mem)\n",
    "print(\"저장 되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940de4d0",
   "metadata": {},
   "source": [
    "# BeatifulSoup로 Scraping하기\n",
    ": 간단하게 HTML과 XML에서 정보를 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80dae4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\chanholee\\anaconda3\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\chanholee\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e574fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>Header</h1>\n",
      "<p> Line 1을 서술 </p>\n",
      "------------\n",
      "h1= Header\n",
      "h1= Header\n",
      "p1  Line 1을 서술 \n",
      "p1  Line 1을 서술 \n"
     ]
    }
   ],
   "source": [
    "# 기본 사용법\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample \n",
    "\n",
    "html = \"\"\"\n",
    "        <html>\n",
    "            <body>\n",
    "                <h1>Header</h1>\n",
    "                <p> Line 1을 서술 </p>\n",
    "            </body>\n",
    "        </html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML분석\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "#print(soup)\n",
    "\n",
    "# 원하는 부분 추출하기 (태그를 통한 스크래핑)\n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "print(h1)\n",
    "print(p1)\n",
    "print(\"------------\")\n",
    "# Text만 출력\n",
    "print(\"h1=\", h1.string)\n",
    "print(\"h1=\", h1.text)\n",
    "print(\"p1\", p1.string)\n",
    "print(\"p1\", p1.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbb3ba91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 id=\"title\"> Header </h1>\n",
      "<p1 id=\"body\"> Line 1을 서술 \n",
      "</p1>\n",
      "title=  Header \n",
      "body=  Line 1을 서술 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# id로 요소를 추출하기\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample\n",
    "\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <h1 id=\"title\"> Header </h1>\n",
    "            <p1 id=\"body\"> Line 1을 서술 </p>\n",
    "        </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML분석\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 원하는 부분 추출하기\n",
    "title = soup.find(id='title')\n",
    "body = soup.find(id=\"body\")\n",
    "print(title)\n",
    "print(body)\n",
    "\n",
    "# Text만 출력\n",
    "print(\"title=\", title.text)\n",
    "print(\"body=\", body.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b469d206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver >>> http://www.naver.com\n",
      "daum >>> http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "# 여러개의 요소 추출하기\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample\n",
    "\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <ul>\n",
    "                <li><a href=\"http://www.naver.com\">naver</a></li>\n",
    "                <li><a href=\"http://www.daum.net\">daum</a></li>\n",
    "            </ul>\n",
    "        </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "# HTML분석\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 원하는 부분 추출하기\n",
    "# soup.find('a')\n",
    "links = soup.find_all('a')\n",
    "\n",
    "# 링크 목록 출력\n",
    "for link in links:\n",
    "#   print(link.string)\n",
    "#    print(link.attrs['href']) #attrs = attribute 태그\n",
    "    \n",
    "    href = link.attrs['href']\n",
    "    text = link.string\n",
    "    print(text, \">>>\", href)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1821a0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기상청 육상 중기예보\n",
      "○ (강수) 16일(금) 오후에는 소나기가 내리는 곳이 있겠고, 19일(월) 오후에는 비가 내리겠습니다.<br />○ (기온) 이번 예보기간 아침최저기온은 23~26도, 낮최고기온은 29~34도로 어제(12일, 아침최저기온 20~24도, 낮최고기온 30~33도)와 비슷하거나 조금 높겠습니다.<br />○ (해상) 서해중부해상의 물결은 0.5~2.0m로 일겠습니다.<br />○ (주말전망) 17일(토)~18일(일)은 구름많겠고, 아침 기온은 23~24도, 낮 기온은 29~33도가 되겠습니다<br /><br />* 이번 예보기간 동안 내륙에는 낮최고기온이 33도 내외, 아침최저기온이 25도 내외로 오르는 곳이 많아 매우 무덥겠으니, 건강관리에 각별히 유의하기 바랍니다.<br />* 또한, 북태평양고기압 위치에 따른 강수 변동성이 크겠으니, 앞으로 발표되는 기상정보를 참고하기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "# 기상청 자료 활용하기\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109\"\n",
    "\n",
    "# data 가져오기\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "# print(soup)\n",
    "\n",
    "\n",
    "# 원하는 데이터 추출하기\n",
    "title = soup.find('title').string\n",
    "print(title)\n",
    "wf = soup.find('wf').string\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c21a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8109b72b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c382c8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b643f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe6894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c58e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a710d08b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
